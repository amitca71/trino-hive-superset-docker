{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4728cd61-05d8-4dad-adec-6bcec59b9baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "from datetime import datetime\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, to_timestamp, lit, hash, array, explode\n",
    "\n",
    "SCD_COLS = [\"start_date\", \"end_date\", \"is_deleted\", \"open_reason\", \"close_reason\"]\n",
    "EOW_DATE_STR = \"2999-12-31 00:00:00\"\n",
    "\n",
    "def column_renamer(df: DataFrame, alias: str, keys_list: List, add_suffix: int = 1) -> DataFrame:\n",
    "    \"\"\" Not applicable to keys_list or SCD_COLS.\n",
    "    \n",
    "    adding/removing suffix f\"_{alias}\" to columns of dataframe DF. Created in order to avoid mismatches on same columns exising in \n",
    "    both history + current DFs. \n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): dataframe to perform renaming on\n",
    "        alias (str): alias to add as suffix\n",
    "        keys_list (List): list of keys which are used for joins\n",
    "        add_suffix (int, optional): 0 - remove suffix, 1- add suffix. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: dataframe with performed adding/removing of suffix\n",
    "    \"\"\"\n",
    "    if add_suffix == 1:\n",
    "        for column in set(df.columns)-set(SCD_COLS)-set(keys_list):\n",
    "            df = df.withColumnRenamed(column, column+\"_\"+alias)\n",
    "    else:\n",
    "        for column in [cols for cols in df.columns if f\"_{alias}\" in cols]:\n",
    "            df = df.withColumnRenamed(column, column.replace(\"_\"+alias, \"\"))\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_values_hash(dataframe: DataFrame, keys_list: List, ignored_columns: List = []) -> DataFrame:\n",
    "    \"\"\" adding a hash column for faster check for updated values. Exluding SCD_COLS and keys_list columns. If there are\n",
    "    only key and SCD columns will use 1 as hash value\n",
    "\n",
    "    Args:\n",
    "        dataframe (DataFrame): dataframe to add \"hash\" column to\n",
    "        keys_list (List): keys_list which will be used for joins\n",
    "        ignored_columns (List): columns which will not be included in hash apart from SCD_COLS and keys_list\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Dataframe with added column \"hash\"\n",
    "    \"\"\"\n",
    "    cols = list(set(dataframe.columns) - set(keys_list)-set(SCD_COLS)-set(ignored_columns))\n",
    "    columns = [col(column) for column in cols]\n",
    "    if columns:\n",
    "        return dataframe.withColumn(\"hash\", hash(*columns))\n",
    "    else:\n",
    "        return dataframe.withColumn(\"hash\", hash(lit(1)))\n",
    "\n",
    "\n",
    "def get_open_and_closed(history: DataFrame) -> Union[DataFrame, DataFrame]:\n",
    "    \"\"\" Splitting historical/end result table to Old closed rows and currently open ones. \n",
    "    Also validating if end_date/close_date is present in the dataframe. If not - throwing an exception\n",
    "\n",
    "    Args:\n",
    "        history (DataFrame): SCD/end table\n",
    "\n",
    "    Returns:\n",
    "        Union[DataFrame, DataFrame]: [open rows DF, closed rows DF]\n",
    "    \"\"\"\n",
    "    col_for_close = \"end_date\"\n",
    "    if \"close_date\" in history.columns:\n",
    "        col_for_close = \"end_date\"\n",
    "    elif \"end_date\" not in history.columns:\n",
    "        Exception(\"No end_date/close_date in the history DF\")\n",
    "\n",
    "    return history.where(col(col_for_close) >= to_timestamp(lit(\"2500-12-31\"))), \\\n",
    "           history.where(col(col_for_close) < to_timestamp(lit(\"2500-12-31\")))\n",
    "\n",
    "def no_change_or_update(history_open: DataFrame,\n",
    "                        current: DataFrame,\n",
    "                        keys_list: List, close_date: str = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")) -> DataFrame:\n",
    "    \"\"\" handling of updated or not changed rows.\n",
    "\n",
    "    Args:\n",
    "        history_open (DataFrame): open SCD rows\n",
    "        current (DataFrame): current status\n",
    "        keys_list (List): keys list used for joins\n",
    "        close_date (str, optional): close_date to use if there is a specific you prefer (maybe ds/next_ds if it's from airflow).\n",
    "        Defaults to datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\").\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Spark DF with handled updated/not changed rows part only\n",
    "    \"\"\"\n",
    "\n",
    "    history_open_hash = column_renamer(get_values_hash(history_open, keys_list), alias=\"history\", keys_list=keys_list)\n",
    "    current_hash = column_renamer(get_values_hash(current, keys_list), alias=\"current\", keys_list=keys_list)\n",
    "\n",
    "    not_changed = column_renamer(history_open_hash\n",
    "                                 .join(other=current_hash, on=keys_list, how=\"inner\")\n",
    "                                 .where(history_open_hash[\"hash_history\"] == current_hash[\"hash_current\"])\n",
    "                                 .drop(*[\"hash_history\", \"hash_current\"])\n",
    "                                 .drop(*[column for column in current_hash.columns if \"_current\" in column])\n",
    "                                 , alias=\"history\", keys_list=keys_list, add_suffix=0).select(history_open.columns)\n",
    "\n",
    "    changed = (history_open_hash\n",
    "               .join(other=current_hash, on=keys_list, how=\"inner\")\n",
    "               .where(history_open_hash[\"hash_history\"] != current_hash[\"hash_current\"])\n",
    "               .withColumn(\"hist_flag\", array([lit(x) for x in [0, 1]]))\n",
    "               .withColumn(\"hist_flag\", explode(col(\"hist_flag\")))\n",
    "               )\n",
    "    upd_closed = (column_renamer((changed.where(\"hist_flag=1\")\n",
    "                                  .withColumn(\"end_date\", to_timestamp(lit(close_date), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "                                 .drop(*[\"hash_history\", \"hash_current\", \"hist_flag\"])\n",
    "                                 .drop(*[column for column in changed.columns if \"_current\" in column])\n",
    "                                 ), alias=\"history\", keys_list=keys_list, add_suffix=0).withColumn(\"close_reason\",\n",
    "                                                                                                   lit(\"changed_value\"))\n",
    "                  .select(history_open.columns))\n",
    "\n",
    "    upd_new = (column_renamer((changed.where(\"hist_flag=0\")\n",
    "                               .withColumn(\"start_date\", to_timestamp(lit(close_date), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "                               .withColumn(\"end_date\", to_timestamp(lit(EOW_DATE_STR), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "                               .drop(*[\"hash_history\", \"hash_current\", \"hist_flag\"])\n",
    "                               .drop(*[column for column in changed.columns if \"_history\" in column])\n",
    "                               ), alias=\"current\", keys_list=keys_list, add_suffix=0)\n",
    "               .withColumn(\"close_reason\", lit(None))\n",
    "               .withColumn(\"open_reason\", lit(\"changed_value\"))\n",
    "               .select(history_open.columns))\n",
    "\n",
    "    return (not_changed\n",
    "            .unionByName(upd_new)\n",
    "            .unionByName(upd_closed))\n",
    "\n",
    "\n",
    "def new_rows(history_open: DataFrame,\n",
    "             current: DataFrame,\n",
    "             keys_list: List,\n",
    "             open_date: str = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "             open_column: str = None) ->DataFrame:\n",
    "    \"\"\"Handling new rows insertion. If open column is supplied that column value will be used\n",
    "    to fill start_date. open_date WILL BE IGNORED\n",
    "\n",
    "    Args:\n",
    "        history_open (DataFrame): SCD open rows DF\n",
    "        current (DataFrame): current state\n",
    "        keys_list (List): keys list for joins\n",
    "        open_date (str, optional): custom open date if needed (i.e. from airflow for backfill).\n",
    "            Defaults to datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\").\n",
    "        open_column (str, optional): column name which is going to be used as start_date in SCD.\n",
    "            By default no column will be used like this.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Only new rows part is returned\n",
    "    \"\"\"\n",
    "    new = current.join(other=history_open, on=keys_list, how=\"left_anti\")\n",
    "    new = (new.withColumn(\"end_date\", to_timestamp(lit(EOW_DATE_STR), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "              .withColumn(\"open_reason\", lit(\"new\"))\n",
    "              .withColumn(\"close_reason\", lit(None))\n",
    "              .withColumn(\"is_deleted\", lit(0)))\n",
    "    if open_column:\n",
    "        return new.withColumn(\"start_date\", col(open_column))\n",
    "    else:\n",
    "        return new.withColumn(\"start_date\", to_timestamp(lit(open_date)))\n",
    "\n",
    "\n",
    "def deleted_rows(history_open: DataFrame,\n",
    "                 current: DataFrame,\n",
    "                 keys_list: List,\n",
    "                 close_date: str = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")) -> DataFrame:\n",
    "    \"\"\"Handling of deleted rows - closing open rows and opening ones with is_deleted flag\n",
    "\n",
    "    Args:\n",
    "        history_open (DataFrame): SCD open rows\n",
    "        current (DataFrame): current state DF\n",
    "        keys_list (List): keys list for joins\n",
    "        close_date (str, optional): Custom close_date can be passed (i.e. from Airflow). Defaults to datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\").\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DF with deleted rows changes\n",
    "    \"\"\"\n",
    "\n",
    "    deleted = history_open.join(other=current, on=keys_list, how=\"left_anti\")\n",
    "    closed_rows = (deleted\n",
    "                   .withColumn(\"end_date\", to_timestamp(lit(close_date), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "                   .withColumn(\"close_reason\", lit(\"deleted\")))\n",
    "    open_close_row = (deleted\n",
    "                      .withColumn(\"start_date\", to_timestamp(lit(close_date), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "                      .withColumn(\"end_date\", to_timestamp(lit(EOW_DATE_STR), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "                      .withColumn(\"open_reason\", lit(\"deleted\"))\n",
    "                      .withColumn(\"close_reason\", lit(None))\n",
    "                      .withColumn(\"is_deleted\", lit(1)))\n",
    "\n",
    "    return closed_rows.unionByName(open_close_row)\n",
    "\n",
    "\n",
    "def scd(history: DataFrame,\n",
    "        current: DataFrame,\n",
    "        keys_list: List,\n",
    "        custom_close_date: str = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")) -> DataFrame:\n",
    "    history_open, history_closed = get_open_and_closed(history)\n",
    "    upd_not_changed = no_change_or_update(history_open=history_open,\n",
    "                                          current=current,\n",
    "                                          keys_list=keys_list,\n",
    "                                          close_date=custom_close_date)\n",
    "\n",
    "    new = new_rows(history_open=history_open,\n",
    "                   current=current,\n",
    "                   keys_list=keys_list,\n",
    "                   open_date=custom_close_date)\n",
    "\n",
    "    deleted = deleted_rows(history_open=history_open,\n",
    "                           current=current,\n",
    "                           keys_list=keys_list,\n",
    "                           close_date=custom_close_date)\n",
    "\n",
    "    return (history_closed\n",
    "            .unionByName(upd_not_changed)\n",
    "            .unionByName(new)\n",
    "            .unionByName(deleted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9db39904-4674-457f-9c5f-a4a7c53d3e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext\n",
    "import sys\n",
    "\n",
    "\n",
    "#from slowly_changing_dimension.lib.scd_component import get_open_and_closed, no_change_or_update, new_rows, deleted_rows\n",
    "from pyspark.sql.functions import col, lit, to_timestamp\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b749f5a-94d5-4461-a5e3-2853d2399865",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "com.databricks#dbutils-api_2.12 added as a dependency\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-0f035a98-ce1d-453b-a2ec-74bc04f4e1ba;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.databricks#dbutils-api_2.12;0.0.5 in central\n",
      "\tfound io.delta#delta-core_2.12;1.0.0 in central\n",
      "\tfound org.antlr#antlr4;4.7 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.7 in central\n",
      "\tfound org.antlr#antlr-runtime;3.5.2 in central\n",
      "\tfound org.antlr#ST4;4.0.8 in central\n",
      "\tfound org.abego.treelayout#org.abego.treelayout.core;1.0.3 in central\n",
      "\tfound org.glassfish#javax.json;1.0.4 in central\n",
      "\tfound com.ibm.icu#icu4j;58.2 in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.2 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.2 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.4.1 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.4-3 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.2 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.819 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.2.0 in central\n",
      "downloading https://repo1.maven.org/maven2/com/databricks/dbutils-api_2.12/0.0.5/dbutils-api_2.12-0.0.5.jar ...\n",
      "\t[SUCCESSFUL ] com.databricks#dbutils-api_2.12;0.0.5!dbutils-api_2.12.jar (151ms)\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-core_2.12/1.0.0/delta-core_2.12-1.0.0.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-core_2.12;1.0.0!delta-core_2.12.jar (3121ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.0.2/spark-sql-kafka-0-10_2.12-3.0.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.2!spark-sql-kafka-0-10_2.12.jar (342ms)\n",
      "downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.819/aws-java-sdk-bundle-1.11.819.jar ...\n",
      "\t[SUCCESSFUL ] com.amazonaws#aws-java-sdk-bundle;1.11.819!aws-java-sdk-bundle.jar (94242ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.2.0/hadoop-aws-3.2.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-aws;3.2.0!hadoop-aws.jar (264ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4/4.7/antlr4-4.7.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4;4.7!antlr4.jar (500ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.7/antlr4-runtime-4.7.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4-runtime;4.7!antlr4-runtime.jar (169ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr-runtime/3.5.2/antlr-runtime-3.5.2.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr-runtime;3.5.2!antlr-runtime.jar (145ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/ST4/4.0.8/ST4-4.0.8.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#ST4;4.0.8!ST4.jar (161ms)\n",
      "downloading https://repo1.maven.org/maven2/org/abego/treelayout/org.abego.treelayout.core/1.0.3/org.abego.treelayout.core-1.0.3.jar ...\n",
      "\t[SUCCESSFUL ] org.abego.treelayout#org.abego.treelayout.core;1.0.3!org.abego.treelayout.core.jar(bundle) (95ms)\n",
      "downloading https://repo1.maven.org/maven2/org/glassfish/javax.json/1.0.4/javax.json-1.0.4.jar ...\n",
      "\t[SUCCESSFUL ] org.glassfish#javax.json;1.0.4!javax.json.jar(bundle) (120ms)\n",
      "downloading https://repo1.maven.org/maven2/com/ibm/icu/icu4j/58.2/icu4j-58.2.jar ...\n",
      "\t[SUCCESSFUL ] com.ibm.icu#icu4j;58.2!icu4j.jar (8654ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.0.2/spark-token-provider-kafka-0-10_2.12-3.0.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.2!spark-token-provider-kafka-0-10_2.12.jar (112ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/2.4.1/kafka-clients-2.4.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;2.4.1!kafka-clients.jar (1893ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.6.2/commons-pool2-2.6.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-pool2;2.6.2!commons-pool2.jar (172ms)\n",
      "downloading https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar ...\n",
      "\t[SUCCESSFUL ] org.spark-project.spark#unused;1.0.0!unused.jar (74ms)\n",
      "downloading https://repo1.maven.org/maven2/com/github/luben/zstd-jni/1.4.4-3/zstd-jni-1.4.4-3.jar ...\n",
      "\t[SUCCESSFUL ] com.github.luben#zstd-jni;1.4.4-3!zstd-jni.jar (2263ms)\n",
      "downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.7.1/lz4-java-1.7.1.jar ...\n",
      "\t[SUCCESSFUL ] org.lz4#lz4-java;1.7.1!lz4-java.jar (372ms)\n",
      "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.8.2/snappy-java-1.1.8.2.jar ...\n",
      "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.8.2!snappy-java.jar(bundle) (1042ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.30!slf4j-api.jar (94ms)\n",
      ":: resolution report :: resolve 23800ms :: artifacts dl 114006ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.819 from central in [default]\n",
      "\tcom.databricks#dbutils-api_2.12;0.0.5 from central in [default]\n",
      "\tcom.github.luben#zstd-jni;1.4.4-3 from central in [default]\n",
      "\tcom.ibm.icu#icu4j;58.2 from central in [default]\n",
      "\tio.delta#delta-core_2.12;1.0.0 from central in [default]\n",
      "\torg.abego.treelayout#org.abego.treelayout.core;1.0.3 from central in [default]\n",
      "\torg.antlr#ST4;4.0.8 from central in [default]\n",
      "\torg.antlr#antlr-runtime;3.5.2 from central in [default]\n",
      "\torg.antlr#antlr4;4.7 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.7 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.2.0 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.0.2 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.2 from central in [default]\n",
      "\torg.glassfish#javax.json;1.0.4 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.2 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.375 by [com.amazonaws#aws-java-sdk-bundle;1.11.819] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   21  |   20  |   20  |   1   ||   20  |   20  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-0f035a98-ce1d-453b-a2ec-74bc04f4e1ba\n",
      "\tconfs: [default]\n",
      "\t20 artifacts copied, 0 already retrieved (198147kB/602ms)\n",
      "21/11/15 14:26:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "E                                                                               \n",
      "======================================================================\n",
      "ERROR: /home/jovyan/ (unittest.loader._FailedTest)\n",
      "----------------------------------------------------------------------\n",
      "AttributeError: module '__main__' has no attribute '/home/jovyan/'\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.001s\n",
      "\n",
      "FAILED (errors=1)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "True",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3452: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "class SCDTest(unittest.TestCase):\n",
    "\n",
    "    ROW_JSON = {\"app_id\": \"13ff8629-c1fc-e289-e81f-bc8c8968e9d6\", \"feature\": 1}\n",
    "    _spark = SparkSession.builder.appName(\"Spark Benchmarking\").master(\"local[*]\").getOrCreate()\n",
    "    _sc = SparkContext.getOrCreate()\n",
    "    current = _spark.read.json(_sc.parallelize([ROW_JSON]))\n",
    "    history = (current\n",
    "               .withColumn(\"start_date\", to_timestamp(lit(\"2000-01-01 00:01:02\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "               .withColumn(\"end_date\", to_timestamp(lit(\"2999-12-31 00:00:00\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "               .withColumn(\"open_reason\", lit(\"new\"))\n",
    "               .withColumn(\"close_reason\", lit(None))\n",
    "               .withColumn(\"is_deleted\", lit(0))\n",
    "               )\n",
    "    keys_list = [\"app_id\"]\n",
    "\n",
    "    def test_no_end__or_close_date_in_history(self):\n",
    "        self.assertRaises(Exception, get_open_and_closed(self.history.drop(col(\"end_date\"))))\n",
    "\n",
    "    def test_updated(self):\n",
    "        open_rows, _ = get_open_and_closed(self.history)\n",
    "        merged = no_change_or_update(history_open=open_rows.withColumn(\"feature\", lit(5)),\n",
    "                                     current=self.current,\n",
    "                                     keys_list=self.keys_list)\n",
    "        self.assertEqual(merged.count(), 2)\n",
    "\n",
    "    def test_no_change(self):\n",
    "        open_rows, _ = get_open_and_closed(self.history)\n",
    "        merged = no_change_or_update(history_open=open_rows,\n",
    "                                     current=self.current,\n",
    "                                     keys_list=self.keys_list)\n",
    "        self.assertEqual(merged.count(), 1)\n",
    "\n",
    "    def test_new_row(self):\n",
    "        open_rows, _ = get_open_and_closed(self.history)\n",
    "        new_current = self.current.union(self.current.withColumn(\"app_id\", lit(\"random_string_for_testing\")))\n",
    "        new_row_df = new_rows(history_open=open_rows,\n",
    "                              current=new_current,\n",
    "                              keys_list=self.keys_list)\n",
    "        self.assertEqual(new_row_df.count(), 1)\n",
    "\n",
    "    def test_delete_row(self):\n",
    "        open_rows, _ = get_open_and_closed(self.history)\n",
    "        # Intentional filter to have an empty dataframe\n",
    "        empty_df = self.current.where(\"app_id=='1asfaf'\")\n",
    "        deleted_rows_df = deleted_rows(history_open=open_rows, current=empty_df, keys_list=self.keys_list)\n",
    "        self.assertEqual(deleted_rows_df.count(), 2)\n",
    "\n",
    "    def test_no_change_no_feature(self):\n",
    "        open_rows, _ = get_open_and_closed(self.history.drop(col(\"feature\")))\n",
    "        merged = no_change_or_update(history_open=open_rows.drop(col(\"feature\")),\n",
    "                                     current=self.current.drop(col(\"feature\")),\n",
    "                                     keys_list=self.keys_list)\n",
    "        self.assertEqual(merged.count(), 1)\n",
    "\n",
    "    def test_new_row_no_feature(self):\n",
    "        open_rows, _ = get_open_and_closed(self.history.drop(col(\"feature\")))\n",
    "        current_v2 = self.current.drop(col(\"feature\")).unionByName(self.current\n",
    "                                                                   .withColumn(\"app_id\", lit(\"RANDOM_COLUMN\")).drop(col(\"feature\")))\n",
    "        merged = new_rows(history_open=open_rows,\n",
    "                          current=current_v2,\n",
    "                          keys_list=self.keys_list)\n",
    "        self.assertEqual(merged.count(), 1)\n",
    "\n",
    "    def test_delete_no_feature(self):\n",
    "        open_rows, _ = get_open_and_closed(self.history.drop(col(\"feature\")))\n",
    "        current_v2 = self.current.withColumn(\"app_id\", lit(\"RANDOM_COLUMN\")).drop(col(\"feature\"))\n",
    "        merged = deleted_rows(history_open=open_rows,\n",
    "                              current=current_v2,\n",
    "                              keys_list=self.keys_list)\n",
    "        self.assertEqual(merged.count(), 2)\n",
    "\n",
    "    def test_new_row_use_date_col(self):\n",
    "        open_rows, _ = get_open_and_closed(self.history)\n",
    "        open_rows = open_rows.where(\"app_id ='TotalGibberish'\")\n",
    "        current_v2 = (self.current\n",
    "                      .withColumn(\"date_created\", to_timestamp(lit(\"2010-01-01 00:01:02\"), \"yyyy-MM-dd HH:mm:ss\")))\n",
    "        merged = new_rows(history_open=open_rows,\n",
    "                          current=current_v2,\n",
    "                          keys_list=self.keys_list,\n",
    "                          open_column=\"date_created\")\n",
    "        self.assertEqual(merged.select(\"start_date\").collect()[0][\"start_date\"], datetime(2010, 1, 1, 0, 1, 2))\n",
    "\n",
    "\n",
    "    unittest.main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa656f18-7117-4526-a9bb-50c9ee077057",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
